{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree**\n"
      ],
      "metadata": {
        "id": "cq90Zb-6jvFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "    - A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences as a tree-like structure. For classification, it repeatedly splits the dataset into subsets based on the value of input features, making decisions at each node. The splitting continues until the data is classified or stopping criteria are met. The output is a tree, where each leaf represents a class label, and each internal node represents a decision rule based on a feature.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "    - Gini Impurity measures the likelihood of incorrect classification of a randomly chosen element in a dataset if it was randomly labeled according to class distribution in that node. It ranges from 0 (perfectly pure) to 0.5 (maximal impurity for binary classification).\n",
        "\n",
        "    - Entropy quantifies the level of disorder, unpredictability, or impurity in the dataset. It is highest when classes are equally mixed and zero when perfectly pure.\n",
        "\n",
        "    - Both these metrics are used to decide how to split the data at each node; splits that reduce impurity the most are preferred, creating purer child nodes, which improves classification accuracy.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "    - Pre-Pruning involves stopping the growth of the tree early—before it perfectly classifies the training data—based on predefined criteria like min_samples_split, max_depth, or min_impurity_decrease. Advantage: Prevents overfitting by keeping the tree simple and generalizable.\n",
        "\n",
        "    - Post-Pruning allows the tree to grow fully and then prunes back some branches based on validation data or performance metrics (e.g., cost complexity pruning). Advantage: Can discover optimal tree structure after evaluating actual performance, potentially increasing accuracy.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "    - Information Gain measures the reduction in impurity (entropy or Gini) after a dataset is split on a feature. It quantifies how well a feature separates classes. The split with the highest information gain is chosen, leading to the most informative child nodes and improving the classification power of the tree.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "    - Applications:Medical diagnosis (predict diseases), Credit approval and fraud detection, Customer segmentation in marketing, Churn prediction, Risk assessment in insurance\n",
        "    \n",
        "    - Advantages:Easy to interpret and visualize, Handles both numerical and categorical data, Requires little data preprocessing\n",
        "\n",
        "    - Limitations:Prone to overfitting, Can be unstable with small changes in data, May yield biased trees if classes are imbalanced"
      ],
      "metadata": {
        "id": "jPPgAyZdj0_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_6m75U-jtoO",
        "outputId": "e4f75a38-7280-4923-c3fd-525e86f0d0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01333333 0.01333333 0.05072262 0.92261071]\n"
          ]
        }
      ],
      "source": [
        "# 6. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", importances)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "clf_full.fit(X, y)\n",
        "clf_depth3.fit(X, y)\n",
        "\n",
        "acc_full = accuracy_score(y, clf_full.predict(X))\n",
        "acc_depth3 = accuracy_score(y, clf_depth3.predict(X))\n",
        "\n",
        "print(\"Fully-grown tree accuracy:\", acc_full)\n",
        "print(\"Max depth 3 accuracy:\", acc_depth3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CIh72ftlg_D",
        "outputId": "605a95a2-ed1c-4955-d1a5-d6987aa3e0e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.0\n",
            "Max depth 3 accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "#  Load the Boston Housing Dataset\n",
        "#  Train a Decision Tree Regressor\n",
        "#  Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Here actually load_boston dataset is removed from sklearn dataset so I have used Fetch_california_housing.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "importances = reg.feature_importances_\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA_WtPtKlqqw",
        "outputId": "c2f77aed-f535-4a06-fb1d-d615c964655d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 9.555001274479309e-32\n",
            "Feature Importances: [0.52468705 0.0510357  0.0536226  0.02716341 0.03212433 0.13153595\n",
            " 0.0938629  0.08596806]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Write a Python program to:\n",
        "#Load the Iris Dataset\n",
        "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "params = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 5, 10]}\n",
        "clf = DecisionTreeClassifier()\n",
        "gs = GridSearchCV(clf, params, cv=5)\n",
        "gs.fit(X, y)\n",
        "\n",
        "print(\"Best parameters:\", gs.best_params_)\n",
        "print(\"Best accuracy:\", gs.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQVtTvA8mUYd",
        "outputId": "44a24e1c-46cb-4931-fd3d-590828829bad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Best accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you're working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting\n",
        "\n",
        "    - Handle missing values: Use imputation (mean/median for numeric, mode or custom for categorical).\n",
        "\n",
        "    - Encode categorical features: Use label encoding or one-hot encoding for non-numeric attributes.\n",
        "\n",
        "    - Train Decision Tree model: Fit to training data.\n",
        "\n",
        "    - Tune hyperparameters: Use GridSearchCV (parameters like max_depth, min_samples_split).\n",
        "\n",
        "    - Evaluate performance: Use metrics like accuracy, precision, recall, F1 score (for classification); also ROC-AUC if relevant.\n",
        "\n",
        "    - Business value: Automating diagnosis improves efficiency and consistency, supports early detection and intervention, and helps prioritize cases needing urgent care."
      ],
      "metadata": {
        "id": "NzoqI6demmIg"
      }
    }
  ]
}